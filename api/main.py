"""
API FastAPI pour le mod√®le de scoring de cr√©dit
"""
import os
import json
import time
import joblib
import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
from api.schemas import ClientData, PredictionResponse, HealthResponse

import cProfile
import pstats
import io

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cr√©ation de l'application FastAPI
app = FastAPI(
    title="API de Scoring de Cr√©dit",
    description="API pour pr√©dire la solvabilit√© des demandes de cr√©dit",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales pour le mod√®le
MODEL = None
MODEL_VERSION = "1.0.0"
LOGS_FILE = "logs/production_logs.json"


def load_model():
    """
    Charge le mod√®le LightGBM au d√©marrage de l'application.
    Le mod√®le est charg√© UNE SEULE FOIS.
    """
    global MODEL, MODEL_FEATURES

    model_path = "models/model.pkl"

    try:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Mod√®le introuvable : {model_path}")

        artifact = joblib.load(model_path)

        # Cas recommand√© : dict {"model": ..., "features": [...]}
        if isinstance(artifact, dict):
            MODEL = artifact["model"]
            MODEL_FEATURES = artifact.get("features")

        else:
            MODEL = artifact
            MODEL_FEATURES = None

        logger.info(f"‚úÖ Mod√®le LightGBM charg√© depuis {model_path}")
        logger.info(f"Type du mod√®le : {type(MODEL)}")

        # -----------------------------
        # üîç Extraction des features
        # -----------------------------
        if MODEL_FEATURES:
            logger.info(f"üìå Features (artifact) : {MODEL_FEATURES}")

        elif hasattr(MODEL, "feature_name_"):
            MODEL_FEATURES = list(MODEL.feature_name_)
            logger.info(f"üìå Features (LightGBM) : {MODEL_FEATURES}")

        else:
            raise RuntimeError(
                "Impossible de r√©cup√©rer les features depuis le mod√®le LightGBM"
            )

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        raise

def log_prediction(client_data: dict, prediction: dict):
    """
    Enregistre les pr√©dictions dans un fichier JSON pour le monitoring
    """
    try:
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "input": client_data,
            "output": prediction,
            "model_version": MODEL_VERSION
        }
        
        # Append au fichier de logs
        with open(LOGS_FILE, "a") as f:
            f.write(json.dumps(log_entry) + "\n")
    except Exception as e:
        logger.error(f"Erreur lors de l'√©criture des logs: {e}")


@app.on_event("startup")
async def startup_event():
    """
    √âv√©nement ex√©cut√© au d√©marrage de l'API
    """
    logger.info("üöÄ D√©marrage de l'API de Scoring...")
    load_model()
    logger.info("‚úÖ API pr√™te √† recevoir des requ√™tes")

'''
@app.get("/", tags=["Root"])
async def root():
    """
    Point d'entr√©e de l'API
    """
    return {
        "message": "API de Scoring de Cr√©dit - Pr√™t √† D√©penser",
        "version": MODEL_VERSION,
        "documentation": "/docs",
        "health": "/health"
    }
'''

@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict(client_data: ClientData, request: Request):
    """
    Effectue une pr√©diction de score de cr√©dit
    Retourne un score de solvabilit√© entre 0 et 1, et une d√©cision.
    """

    # V√©rifier que le mod√®le est charg√©
    if MODEL is None:
        raise HTTPException(status_code=503, detail="Mod√®le non disponible")
    
    try:

        pr = cProfile.Profile()
        pr.enable()

        # Mesurer le temps d'inf√©rence
        start_time = time.time()
        
        # Pr√©parer les donn√©es pour la pr√©diction
        features = pd.DataFrame([client_data.model_dump()])
        
        # Pr√©diction
        try:
            # Essayer avec predict_proba (pour les classifieurs)
            probas = MODEL.predict_proba(features)
            score = float(probas[0][1])  # Probabilit√© de la classe positive
        except AttributeError:
            # Si pas de predict_proba, utiliser predict
            prediction = MODEL.predict(features)
            score = float(prediction[0])
        
        # Confidence bas√©e sur le seuil m√©tier (simple et utile)
        confidence = abs(score - 0.2) / 0.8
        confidence = min(confidence, 1.0)
        decision = "APPROVED" if score >= 0.2 else "REJECTED"
        
        # Temps d'inf√©rence
        inference_time = (time.time() - start_time) * 1000  # en ms
        
        # G√©n√©rer un ID unique pour la requ√™te
        client_id = f"req_{datetime.now().strftime('%Y%m%d%H%M%S%f')}"
        
        # Pr√©parer la r√©ponse
        response = PredictionResponse(
            client_id=client_id,
            score=round(score, 4),
            decision=decision,
            confidence=round(confidence, 4),
            inference_time_ms=round(inference_time, 2)
        )
        
        # Logger la pr√©diction
        log_prediction(
            client_data=client_data.model_dump(),
            prediction=response.model_dump()
        )
        
        logger.info(f"‚úÖ Pr√©diction r√©ussie: {client_id} - Score: {score:.2f} - D√©cision: {decision}")
        
        pr.disable()
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats("cumulative")
        ps.print_stats(20)  # top 20 fonctions
        logger.info(s.getvalue())
        pr.dump_stats("output/predict.prof")

        return response
    


    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction: {str(e)}")


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    V√©rifie l'√©tat de sant√© de l'API
    """
    return HealthResponse(
        status="healthy" if MODEL is not None else "unhealthy",
        model_loaded=MODEL is not None,
        version=MODEL_VERSION
    )


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    Gestionnaire global d'exceptions
    """
    logger.error(f"Erreur non g√©r√©e: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Erreur interne du serveur"}
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
