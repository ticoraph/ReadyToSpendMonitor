"""
API FastAPI pour le mod√®le de scoring de cr√©dit
"""
import os
import json
import time
import joblib
import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
from api.schemas import ClientData, PredictionResponse, HealthResponse
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor
from typing import List

import cProfile
import pstats
import io

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Variables globales pour le mod√®le
MODEL = None
MODEL_VERSION = "1.0.0"
LOGS_FILE = "logs/production_logs.json"
EXECUTOR = ThreadPoolExecutor(max_workers=10)  # 10 workers parall√®les

# ========== Configuration du profiling ==========
ENABLE_PROFILING = False  # üëà Modifier pour activer/d√©sactiver le profiling
PROFILE_OUTPUT_DIR = "output"

# Cr√©er le dossier output si n√©cessaire
os.makedirs(PROFILE_OUTPUT_DIR, exist_ok=True)

def load_model():
    """
    Charge le mod√®le LightGBM au d√©marrage de l'application.
    Le mod√®le est charg√© UNE SEULE FOIS.
    """
    global MODEL, MODEL_FEATURES

    model_path = "models/model.pkl"

    try:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Mod√®le introuvable : {model_path}")

        artifact = joblib.load(model_path)

        # Cas recommand√© : dict {"model": ..., "features": [...]}
        if isinstance(artifact, dict):
            MODEL = artifact["model"]
            MODEL_FEATURES = artifact.get("features")

        else:
            MODEL = artifact
            MODEL_FEATURES = None

        logger.info(f"‚úÖ Mod√®le LightGBM charg√© depuis {model_path}")
        logger.info(f"Type du mod√®le : {type(MODEL)}")

        # -----------------------------
        # üîç Extraction des features
        # -----------------------------
        if MODEL_FEATURES:
            logger.info(f"üìå Features (artifact) : {MODEL_FEATURES}")

        elif hasattr(MODEL, "feature_name_"):
            MODEL_FEATURES = list(MODEL.feature_name_)
            logger.info(f"üìå Features (LightGBM) : {MODEL_FEATURES}")

        else:
            raise RuntimeError(
                "Impossible de r√©cup√©rer les features depuis le mod√®le LightGBM"
            )

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        raise

def log_prediction(client_data: dict, prediction: dict):
    """
    Enregistre les pr√©dictions dans un fichier JSON pour le monitoring
    """
    try:
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "input": client_data,
            "output": prediction,
            "model_version": MODEL_VERSION
        }
        
        # Append au fichier de logs
        with open(LOGS_FILE, "a") as f:
            f.write(json.dumps(log_entry) + "\n")
    except Exception as e:
        logger.error(f"Erreur lors de l'√©criture des logs: {e}")

def profile_function(func):
    """
    D√©corateur pour profiler une fonction avec cProfile.
    Active/d√©sactive le profiling selon la variable ENABLE_PROFILING.
    Sauvegarde les stats dans un fichier .prof et les affiche dans les logs.
    """
    from functools import wraps
    
    @wraps(func)  # ‚úÖ Important pour pr√©server les m√©tadonn√©es de la fonction
    async def wrapper(*args, **kwargs):
        if not ENABLE_PROFILING:
            # Si le profiling est d√©sactiv√©, ex√©cuter la fonction normalement
            return await func(*args, **kwargs)
        
        # Profiling activ√©
        pr = cProfile.Profile()
        pr.enable()
        
        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            pr.disable()
            
            # G√©n√©rer les stats
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s).sort_stats("cumulative")
            ps.print_stats(20)  # top 20 fonctions
            
            # Afficher dans les logs
            logger.info(f"\nüìä Profiling Results for {func.__name__}:\n{s.getvalue()}")
            
            # Sauvegarder le fichier .prof
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            profile_file = os.path.join(PROFILE_OUTPUT_DIR, f"{func.__name__}_{timestamp}.prof")
            pr.dump_stats(profile_file)
            logger.info(f"‚úÖ Profiling stats sauvegard√©es dans {profile_file}")
    
    return wrapper

def _process_single_prediction(client_data: ClientData, model):
    """
    Fonction interne pour traiter une seule pr√©diction
    (utilis√©e par le ThreadPoolExecutor)
    """
    try:
        # Mesurer le temps d'inf√©rence
        start_time = time.time()
        
        # Pr√©parer les donn√©es pour la pr√©diction
        features = pd.DataFrame([client_data.model_dump()])
        
        # Pr√©diction
        try:
            # Essayer avec predict_proba (pour les classifieurs)
            probas = model.predict_proba(features)
            score = float(probas[0][1])  # Probabilit√© de la classe positive
        except AttributeError:
            # Si pas de predict_proba, utiliser predict
            prediction = model.predict(features)
            score = float(prediction[0])
        
        # Confidence bas√©e sur le seuil m√©tier (simple et utile)
        confidence = abs(score - 0.2) / 0.8
        confidence = min(confidence, 1.0)
        decision = "APPROVED" if score >= 0.2 else "REJECTED"
        
        # Temps d'inf√©rence
        inference_time = (time.time() - start_time) * 1000  # en ms
        
        # G√©n√©rer un ID unique pour la requ√™te
        client_id = f"req_{datetime.now().strftime('%Y%m%d%H%M%S%f')}"
        
        # Pr√©parer la r√©ponse
        response = PredictionResponse(
            client_id=client_id,
            score=round(score, 4),
            decision=decision,
            confidence=round(confidence, 4),
            inference_time_ms=round(inference_time, 2)
        )
        
        # Logger la pr√©diction
        log_prediction(
            client_data=client_data.model_dump(),
            prediction=response.model_dump()
        )
        
        logger.info(f"‚úÖ Pr√©diction r√©ussie: {client_id} - Score: {score:.2f} - D√©cision: {decision}")
        
        return response

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
        raise

@asynccontextmanager
async def lifespan(app: FastAPI):

    logger.info("üöÄ D√©marrage de l'API de Scoring...")
    load_model()
    logger.info("‚úÖ API pr√™te √† recevoir des requ√™tes")
    app.state.model = MODEL
    yield
    
    # Shutdown propre du executor
    logger.info("üõë Arr√™t de l'API...")
    EXECUTOR.shutdown(wait=True)
    logger.info("‚úÖ API arr√™t√©e")

# Cr√©ation de l'application FastAPI
app = FastAPI(
    title="API de Scoring de Cr√©dit",
    description="API pour pr√©dire la solvabilit√© des demandes de cr√©dit",
    version="1.0.0",
    lifespan=lifespan   # üëà ICI
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
@profile_function
async def predict(client_data: ClientData, request: Request):
    """
    Effectue une pr√©diction de score de cr√©dit
    Retourne un score de solvabilit√© entre 0 et 1, et une d√©cision.
    
    üéØ Le profiling cProfile peut √™tre activ√©/d√©sactiv√© avec la variable ENABLE_PROFILING
    """
    model = request.app.state.model

    try:
        # Mesurer le temps d'inf√©rence
        start_time = time.time()
        
        # Pr√©parer les donn√©es pour la pr√©diction
        features = pd.DataFrame([client_data.model_dump()])
        
        # Pr√©diction
        try:
            # Essayer avec predict_proba (pour les classifieurs)
            probas = model.predict_proba(features)
            score = float(probas[0][1])  # Probabilit√© de la classe positive
        except AttributeError:
            # Si pas de predict_proba, utiliser predict
            prediction = model.predict(features)
            score = float(prediction[0])
        
        # Confidence bas√©e sur le seuil m√©tier (simple et utile)
        confidence = abs(score - 0.2) / 0.8
        confidence = min(confidence, 1.0)
        decision = "APPROVED" if score >= 0.2 else "REJECTED"
        
        # Temps d'inf√©rence
        inference_time = (time.time() - start_time) * 1000  # en ms
        
        # G√©n√©rer un ID unique pour la requ√™te
        client_id = f"req_{datetime.now().strftime('%Y%m%d%H%M%S%f')}"
        
        # Pr√©parer la r√©ponse
        response = PredictionResponse(
            client_id=client_id,
            score=round(score, 4),
            decision=decision,
            confidence=round(confidence, 4),
            inference_time_ms=round(inference_time, 2)
        )
        
        # Logger la pr√©diction
        log_prediction(
            client_data=client_data.model_dump(),
            prediction=response.model_dump()
        )
        
        logger.info(f"‚úÖ Pr√©diction r√©ussie: {client_id} - Score: {score:.2f} - D√©cision: {decision}")

        return response
    

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction: {str(e)}")


@app.post("/predict_batch", response_model=List[PredictionResponse], tags=["Prediction"])
async def predict_batch(clients_data: List[ClientData], request: Request):
    """
    Effectue des pr√©dictions en parall√®le pour plusieurs clients.
    Utilise un ThreadPoolExecutor pour paralleliser les requ√™tes.
    
    Exemple de payload:
    [
        {"feature1": 1.0, "feature2": 2.0, ...},
        {"feature1": 3.0, "feature2": 4.0, ...},
        ...
    ]
    """
    try:
        # R√©cup√©rer le mod√®le depuis l'app state
        model = request.app.state.model
        
        # V√©rifier que le mod√®le est charg√©
        if model is None:
            raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
        
        # Soumettre toutes les t√¢ches au thread pool
        futures = [
            EXECUTOR.submit(_process_single_prediction, client, model)
            for client in clients_data
        ]
        
        # Attendre les r√©sultats
        responses = []
        for future in futures:
            try:
                response = future.result()
                responses.append(response)
            except Exception as e:
                logger.error(f"Erreur lors du traitement batch: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Erreur batch: {str(e)}")
        
        logger.info(f"‚úÖ Batch trait√©: {len(responses)} pr√©dictions r√©ussies")
        return responses

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur de pr√©diction batch: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction batch: {str(e)}")


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    V√©rifie l'√©tat de sant√© de l'API
    """
    return HealthResponse(
        status="healthy" if MODEL is not None else "unhealthy",
        model_loaded=MODEL is not None,
        version=MODEL_VERSION
    )


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    Gestionnaire global d'exceptions
    """
    logger.error(f"Erreur non g√©r√©e: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Erreur interne du serveur"}
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)